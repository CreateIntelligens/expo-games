import logging
import os
import threading
import time
import tempfile
from collections import deque
from enum import Enum
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from types import SimpleNamespace

import cv2
import numpy as np

from ..utils.gpu_runtime import configure_gpu_runtime

_GPU_STATUS = configure_gpu_runtime()

# MediaPipe ä¾è³´åˆå§‹åŒ–ï¼ˆGPU åŠ é€Ÿè¨­å®šåœ¨ backend.utils.gpu_runtime ä¸­è™•ç†ï¼‰
try:  # pragma: no cover - åŒ¯å…¥ç‹€æ…‹ä¾è³´åŸ·è¡Œç’°å¢ƒ
    import mediapipe as mp

    _MEDIAPIPE_AVAILABLE = True
    _MEDIAPIPE_ERROR: Optional[str] = None
except Exception as exc:  # pragma: no cover - åŒ¯å…¥å¤±æ•—æ™‚æä¾›é€€å›æ–¹æ¡ˆ
    mp = SimpleNamespace(solutions=SimpleNamespace(face_mesh=None, hands=None))
    _MEDIAPIPE_AVAILABLE = False
    _MEDIAPIPE_ERROR = str(exc)

# TensorFlow è¨˜æ†¶é«”é…ç½® - å„ªå…ˆä½¿ç”¨ CPU é¿å… GPU OOM
try:
    import tensorflow as tf
    import os

    # ç’°å¢ƒè®Šæ•¸é…ç½®
    os.environ['TF_DISABLE_TENSORBOARD'] = '1'
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # æ¸›å°‘æ—¥èªŒè¼¸å‡º

    # æª¢æŸ¥æ˜¯å¦å¼·åˆ¶ä½¿ç”¨ CPU
    force_cpu = os.environ.get('EMOTION_FORCE_CPU', 'true').lower() == 'true'

    if force_cpu:
        print("ğŸ”„ å¼·åˆ¶ä½¿ç”¨ CPU æ¨¡å¼ä»¥é¿å… GPU è¨˜æ†¶é«”å•é¡Œ")
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # éš±è—æ‰€æœ‰ GPU
        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'
    else:
        print("ğŸ® å˜—è©¦ä½¿ç”¨ GPU æ¨¡å¼")
        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        # GPU è¨˜æ†¶é«”æ§åˆ¶
        os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'

    gpus = tf.config.list_physical_devices('GPU')
    if gpus and not force_cpu:
        try:
            # å˜—è©¦è¨­å®š GPU è¨˜æ†¶é«”é™åˆ¶
            for gpu in gpus:
                tf.config.set_logical_device_configuration(
                    gpu,
                    [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]  # é™åˆ° 2GB
                )

            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)

            print(f"âœ… GPU è¨˜æ†¶é«”é™åˆ¶å·²è¨­å®š (2GB): {len(gpus)} å€‹ GPU å¯ç”¨")

            # ç¢ºèª GPU çœŸæ­£å¯ç”¨æ–¼é‹ç®—
            with tf.device('/GPU:0'):
                test_tensor = tf.constant([1.0, 2.0])
                result = tf.square(test_tensor)
                print(f"âœ… GPU é‹ç®—æ¸¬è©¦é€šé: {result.numpy()}")

            # åˆ—å‡º TensorFlow æ­£åœ¨ä½¿ç”¨çš„è¨­å‚™
            print(f"ğŸ“Š å¯ç”¨çš„ç‰©ç†è¨­å‚™: {[dev.name for dev in tf.config.list_physical_devices()]}")
            print(f"ğŸ¯ é‚è¼¯GPUè¨­å‚™: {[dev.name for dev in tf.config.list_logical_devices('GPU')]}")

        except RuntimeError as e:
            print(f"âš ï¸ GPU è¨˜æ†¶é«”é…ç½®å¤±æ•—: {e}")
    else:
        print("â„¹ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU")

except ImportError:
    print("âš ï¸ TensorFlow æœªå®‰è£")

# DeepFace ä¾è³´åˆå§‹åŒ– (äººè‡‰æƒ…ç·’åˆ†æ)
_EMOTION_MODEL = None
try:
    from deepface import DeepFace
    # æ³¨æ„ï¼šæ–°ç‰ˆDeepFace (0.0.85+) å¯èƒ½ç§»é™¤äº†build_modelæ–¹æ³•
    # å…ˆæª¢æŸ¥DeepFaceæ˜¯å¦å¯ç”¨ï¼Œæ¨¡å‹å°‡åœ¨ç¬¬ä¸€æ¬¡èª¿ç”¨æ™‚è‡ªå‹•åŠ è¼‰
    _DEEPFACE_AVAILABLE = True
    _DEEPFACE_ERROR: Optional[str] = None
    logging.info("DeepFace å·²å°±ç·’ï¼Œæ¨¡å‹å°‡åœ¨é¦–æ¬¡ä½¿ç”¨æ™‚åŠ è¼‰")
except Exception as exc:
    DeepFace = None
    _EMOTION_MODEL = None
    _DEEPFACE_AVAILABLE = False
    _DEEPFACE_ERROR = str(exc)
    logging.warning(f"DeepFace ä¸å¯ç”¨: {exc}")

from .status_broadcaster import StatusBroadcaster
from ..utils.datetime_utils import _now_ts


logger = logging.getLogger(__name__)

if _GPU_STATUS.warnings:
    for warning in _GPU_STATUS.warnings:
        logger.warning("GPU setup warning: %s", warning)
else:
    logger.info(
        "GPU runtime ready | TensorFlow devices: %s | MediaPipe GPU enabled: %s",
        _GPU_STATUS.tensorflow_devices,
        _GPU_STATUS.mediapipe_gpu_enabled,
    )


class EmotionType(Enum):
    """æƒ…ç·’é¡å‹æšèˆ‰"""
    HAPPY = "é–‹å¿ƒ"
    SAD = "æ‚²å‚·"
    ANGRY = "ç”Ÿæ°£"
    SURPRISED = "é©šè¨"
    FEARFUL = "ææ‡¼"
    DISGUSTED = "å­æƒ¡"
    NEUTRAL = "ä¸­æ€§"


# æƒ…ç·’ä¸­è‹±æ–‡å°ç…§è¡¨
EMOTION_TRANSLATIONS = {
    "é–‹å¿ƒ": {"en": "happy", "zh": "é–‹å¿ƒ", "emoji": "ğŸ˜Š"},
    "æ‚²å‚·": {"en": "sad", "zh": "æ‚²å‚·", "emoji": "ğŸ˜¢"},
    "ç”Ÿæ°£": {"en": "angry", "zh": "ç”Ÿæ°£", "emoji": "ğŸ˜ "},
    "é©šè¨": {"en": "surprise", "zh": "é©šè¨", "emoji": "ğŸ˜²"},
    "ææ‡¼": {"en": "fear", "zh": "ææ‡¼", "emoji": "ğŸ˜¨"},
    "å­æƒ¡": {"en": "disgust", "zh": "å­æƒ¡", "emoji": "ğŸ¤¢"},
    "ä¸­æ€§": {"en": "neutral", "zh": "é¢ç„¡è¡¨æƒ…", "emoji": "ğŸ˜"}
}


class FacialFeatureExtractor:
    """è‡‰éƒ¨ç‰¹å¾µæå–å™¨ï¼ŒåŸºæ–¼ MediaPipe FaceMesh."""

    def __init__(self):
        self.mediapipe_ready = _MEDIAPIPE_AVAILABLE
        self.init_error: Optional[str] = _MEDIAPIPE_ERROR
        self.face_mesh_stream = None
        self.face_mesh_static = None
        self.face_mesh_lock = threading.Lock()

        # MediaPipe 468é»äººè‡‰ç¶²æ ¼é—œéµç´¢å¼•
        self.LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
        self.RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
        self.MOUTH_INDICES = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312, 13, 82, 81, 80, 16]
        self.EYEBROW_LEFT_INDICES = [70, 63, 105, 66, 107, 55, 65]
        self.EYEBROW_RIGHT_INDICES = [296, 334, 293, 300, 276, 283, 282]
        self.NOSE_TIP = 1
        self.CHIN = 175

        if self.mediapipe_ready:
            try:
                self.mp_face_mesh = mp.solutions.face_mesh
                # å‹•æ…‹ä¸²æµæƒ…å¢ƒï¼ˆæ”å½±æ©Ÿ/å½±ç‰‡ï¼‰
                self.face_mesh_stream = self.mp_face_mesh.FaceMesh(
                    static_image_mode=False,
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5,
                )
                # éœæ…‹åœ–ç‰‡æƒ…å¢ƒ
                self.face_mesh_static = self.mp_face_mesh.FaceMesh(
                    static_image_mode=True,
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5,
                )
                logger.info("MediaPipe FaceMesh åˆå§‹åŒ–å®Œæˆï¼Œå•Ÿç”¨çœŸå¯¦æƒ…ç·’æª¢æ¸¬")
            except Exception as exc:  # pragma: no cover - åˆå§‹åŒ–å¤±æ•—æ™‚è¨˜éŒ„
                self.mediapipe_ready = False
                self.init_error = str(exc)
                logger.exception("åˆå§‹åŒ– MediaPipe FaceMesh å¤±æ•—: %s", exc)
        else:
            logger.warning("MediaPipe FaceMesh ç„¡æ³•ä½¿ç”¨: %s", self.init_error)

    def is_available(self) -> bool:
        """å›å‚³ MediaPipe æ˜¯å¦å¯ç”¨ã€‚"""
        return self.mediapipe_ready and self.face_mesh_stream is not None and self.face_mesh_static is not None

    def extract_features(self, frame, static_image: bool = False) -> Optional[Dict]:
        """å¾å½±åƒå¹€ä¸­æå–è‡‰éƒ¨ç‰¹å¾µã€‚"""
        if frame is None or not self.is_available():
            return None

        height, width = frame.shape[:2]
        mesh = self.face_mesh_static if static_image else self.face_mesh_stream
        if mesh is None:
            return None

        # MediaPipe éœ€è¦ RGB å½±åƒ
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        with self.face_mesh_lock:
            results = mesh.process(frame_rgb)

        if not results or not results.multi_face_landmarks:
            return None

        landmarks = results.multi_face_landmarks[0].landmark
        if len(landmarks) < 468:
            return None

        # å°‡æ¨™è¨˜è½‰æ›ç‚ºåƒç´ åº§æ¨™ï¼Œä¸¦ä¿ç•™æ·±åº¦è³‡è¨Šç”¨æ–¼å°ç¨±æ€§åˆ†æ
        points = [(lm.x * width, lm.y * height, lm.z * width) for lm in landmarks]

        face_width = abs(points[454][0] - points[234][0]) if len(points) > 454 else float(width)
        face_height = abs(points[152][1] - points[10][1]) if len(points) > 152 else float(height)

        if face_width <= 0:
            face_width = float(width)
        if face_height <= 0:
            face_height = float(height)

        features = {
            "eye_aspect_ratio_left": self._calculate_eye_aspect_ratio(points, self.LEFT_EYE_INDICES),
            "eye_aspect_ratio_right": self._calculate_eye_aspect_ratio(points, self.RIGHT_EYE_INDICES),
            "mouth_aspect_ratio": self._calculate_mouth_aspect_ratio(points),
            "mouth_curvature": self._calculate_mouth_curvature(points, face_height),
            "eyebrow_height_left": self._calculate_eyebrow_relative_height(points, self.EYEBROW_LEFT_INDICES, self.LEFT_EYE_INDICES, face_height),
            "eyebrow_height_right": self._calculate_eyebrow_relative_height(points, self.EYEBROW_RIGHT_INDICES, self.RIGHT_EYE_INDICES, face_height),
            "nose_wrinkle": self._calculate_nose_wrinkle(points, face_width),
            "eye_openness": self._calculate_eye_openness(points),
            "mouth_width": self._calculate_mouth_width(points, face_width),
            "facial_symmetry": self._calculate_facial_symmetry(points),
        }

        return features

    def _calculate_eye_aspect_ratio(self, landmarks: List[Tuple], eye_indices: List[int]) -> float:
        """è¨ˆç®—çœ¼éƒ¨é•·å¯¬æ¯” (EAR)"""
        try:
            if len(eye_indices) < 6:
                return 0.3

            eye_points = [landmarks[i] for i in eye_indices[:6]]

            # å‚ç›´è·é›¢
            A = np.linalg.norm(np.array(eye_points[1]) - np.array(eye_points[5]))
            B = np.linalg.norm(np.array(eye_points[2]) - np.array(eye_points[4]))

            # æ°´å¹³è·é›¢
            C = np.linalg.norm(np.array(eye_points[0]) - np.array(eye_points[3]))

            if C == 0:
                return 0.3

            ear = (A + B) / (2.0 * C)
            return ear
        except:
            return 0.3

    def _calculate_mouth_aspect_ratio(self, landmarks: List[Tuple]) -> float:
        """è¨ˆç®—å˜´éƒ¨é•·å¯¬æ¯” (MAR)"""
        try:
            mouth_points = [landmarks[i] for i in self.MOUTH_INDICES[:6]]

            # å‚ç›´è·é›¢
            A = np.linalg.norm(np.array(mouth_points[1]) - np.array(mouth_points[5]))
            B = np.linalg.norm(np.array(mouth_points[2]) - np.array(mouth_points[4]))

            # æ°´å¹³è·é›¢
            C = np.linalg.norm(np.array(mouth_points[0]) - np.array(mouth_points[3]))

            if C == 0:
                return 0.1

            mar = (A + B) / (2.0 * C)
            return mar
        except:
            return 0.1

    def _calculate_mouth_curvature(self, landmarks: List[Tuple], face_height: float) -> float:
        """è¨ˆç®—å˜´è§’å½æ›²åº¦ï¼ˆä»¥è‡‰éƒ¨é«˜åº¦æ­£è¦åŒ–ï¼‰ã€‚"""
        try:
            left_corner = landmarks[78]
            right_corner = landmarks[308]
            top_lip = landmarks[13]
            bottom_lip = landmarks[14]

            mouth_center_y = (top_lip[1] + bottom_lip[1]) / 2
            left_height = mouth_center_y - left_corner[1]
            right_height = mouth_center_y - right_corner[1]

            curvature = (left_height + right_height) / 2
            normalized = curvature / max(face_height, 1e-6)
            return float(normalized * 100)
        except Exception:
            return 0.0

    def _calculate_eyebrow_relative_height(
        self,
        landmarks: List[Tuple],
        eyebrow_indices: List[int],
        eye_indices: List[int],
        face_height: float,
    ) -> float:
        """è¨ˆç®—çœ‰æ¯›ç›¸å°æ–¼çœ¼ç›çš„é«˜åº¦å·®ã€‚"""
        try:
            eyebrow_points = [landmarks[i] for i in eyebrow_indices]
            eyebrow_center_y = sum(p[1] for p in eyebrow_points) / len(eyebrow_points)

            eye_points = [landmarks[i] for i in eye_indices[:6]]
            eye_center_y = sum(p[1] for p in eye_points) / len(eye_points)

            relative_height = (eye_center_y - eyebrow_center_y) / max(face_height, 1e-6)
            return float(relative_height * 100)
        except Exception:
            return 0.0

    def _calculate_nose_wrinkle(self, landmarks: List[Tuple], face_width: float) -> float:
        """è¨ˆç®—é¼»å­çšºç´‹ (é¼»ç¿¼è®ŠåŒ–)"""
        try:
            left_nostril = landmarks[31]
            right_nostril = landmarks[35]

            # è¨ˆç®—é¼»ç¿¼å¯¬åº¦
            nostril_width = abs(right_nostril[0] - left_nostril[0])

            # æ­£å¸¸åŒ–
            if face_width == 0:
                return 0.0

            normalized_width = nostril_width / face_width
            return float(normalized_width)
        except Exception:
            return 0.0

    def _calculate_eye_openness(self, landmarks: List[Tuple]) -> float:
        """è¨ˆç®—çœ¼ç›é–‹åˆåº¦"""
        left_ear = self._calculate_eye_aspect_ratio(landmarks, self.LEFT_EYE_INDICES)
        right_ear = self._calculate_eye_aspect_ratio(landmarks, self.RIGHT_EYE_INDICES)
        return (left_ear + right_ear) / 2

    def _calculate_mouth_width(self, landmarks: List[Tuple], face_width: float) -> float:
        """è¨ˆç®—å˜´å·´å¯¬åº¦"""
        try:
            left_corner = landmarks[78]
            right_corner = landmarks[308]
            mouth_width = abs(right_corner[0] - left_corner[0])

            # æ­£å¸¸åŒ–
            if face_width == 0:
                return 0.0

            normalized_width = mouth_width / face_width
            return float(normalized_width)
        except Exception:
            return 0.0

    def _calculate_facial_symmetry(self, landmarks: List[Tuple]) -> float:
        """è¨ˆç®—è‡‰éƒ¨å°ç¨±æ€§"""
        try:
            # è¨ˆç®—è‡‰éƒ¨ä¸­ç·š
            nose_tip = landmarks[self.NOSE_TIP]
            chin = landmarks[self.CHIN]
            forehead = landmarks[10]

            face_center_x = (nose_tip[0] + chin[0] + forehead[0]) / 3

            # è¨ˆç®—å·¦å³å°ç¨±é»çš„è·é›¢å·®ç•°
            symmetry_points = [
                (landmarks[33], landmarks[362]),    # çœ¼è§’
                (landmarks[78], landmarks[308]),    # å˜´è§’
                (landmarks[234], landmarks[454])    # è‡‰é °
            ]

            asymmetry_sum = 0
            for left_point, right_point in symmetry_points:
                left_dist = abs(left_point[0] - face_center_x)
                right_dist = abs(right_point[0] - face_center_x)
                asymmetry_sum += abs(left_dist - right_dist)

            # æ­£å¸¸åŒ–å°ç¨±æ€§åˆ†æ•¸ (è¼ƒä½çš„å€¼è¡¨ç¤ºæ›´å°ç¨±)
            face_width = abs(landmarks[454][0] - landmarks[234][0])
            if face_width == 0:
                return 0.5

            symmetry_score = 1.0 - (asymmetry_sum / (face_width * len(symmetry_points)))
            return max(0.0, min(1.0, symmetry_score))
        except:
            return 0.5


class EmotionDetector:
    """æƒ…ç·’æª¢æ¸¬å™¨"""

    def __init__(self):
        self.emotion_history = deque(maxlen=10)  # ä¿å­˜æœ€è¿‘10æ¬¡æª¢æ¸¬çµæœ
        self.latest_scores: Dict[str, float] = {}

        # æƒ…ç·’æª¢æ¸¬é–¾å€¼ (å¯æ ¹æ“šå¯¦éš›æ¸¬è©¦èª¿æ•´)
        self.thresholds = {
            'smile_curvature': 5.0,
            'surprise_eyebrow': -10.0,
            'eye_openness_high': 0.4,
            'eye_openness_low': 0.15,
            'mouth_open_surprise': 0.15,
            'mouth_open_sad': 0.05
        }

    def detect_emotion(self, features: Dict) -> Tuple[EmotionType, float]:
        """æª¢æ¸¬æƒ…ç·’ä¸¦è¿”å›æƒ…ç·’é¡å‹å’Œä¿¡å¿ƒåº¦"""
        if not features:
            return EmotionType.NEUTRAL, 0.5

        # æå–é—œéµç‰¹å¾µ
        mouth_curvature = features.get('mouth_curvature', 0)
        eye_openness = features.get('eye_openness', 0.3)
        mouth_aspect_ratio = features.get('mouth_aspect_ratio', 0.1)
        mouth_width = features.get('mouth_width', 0.0)
        eyebrow_left = features.get('eyebrow_height_left', 0)
        eyebrow_right = features.get('eyebrow_height_right', 0)
        nose_wrinkle = features.get('nose_wrinkle', 0)

        # æƒ…ç·’åˆ†æ•¸
        emotion_scores = {
            EmotionType.HAPPY: 0.0,
            EmotionType.SAD: 0.0,
            EmotionType.ANGRY: 0.0,
            EmotionType.SURPRISED: 0.0,
            EmotionType.FEARFUL: 0.0,
            EmotionType.DISGUSTED: 0.0,
            EmotionType.NEUTRAL: 0.5
        }

        # é–‹å¿ƒæª¢æ¸¬ (å¾®ç¬‘)
        if mouth_curvature > self.thresholds['smile_curvature']:
            emotion_scores[EmotionType.HAPPY] += 0.6
        if mouth_width > 0.15:  # å˜´å·´è®Šå¯¬
            emotion_scores[EmotionType.HAPPY] += 0.3
        if eye_openness > 0.25 and eye_openness < 0.4:  # çœ¼ç›å¾®ç‡
            emotion_scores[EmotionType.HAPPY] += 0.1

        # é©šè¨æª¢æ¸¬
        if eye_openness > self.thresholds['eye_openness_high']:
            emotion_scores[EmotionType.SURPRISED] += 0.4
        if mouth_aspect_ratio > self.thresholds['mouth_open_surprise']:
            emotion_scores[EmotionType.SURPRISED] += 0.3
        average_eyebrow = (eyebrow_left + eyebrow_right) / 2
        if average_eyebrow < self.thresholds['surprise_eyebrow']:  # çœ‰æ¯›ä¸Šæš
            emotion_scores[EmotionType.SURPRISED] += 0.3

        # æ‚²å‚·æª¢æ¸¬
        if mouth_curvature < -3.0:  # å˜´è§’ä¸‹å‚
            emotion_scores[EmotionType.SAD] += 0.4
        if eye_openness < self.thresholds['eye_openness_low']:  # çœ¼ç›åŠé–‰
            emotion_scores[EmotionType.SAD] += 0.3
        if mouth_aspect_ratio < self.thresholds['mouth_open_sad']:  # å˜´å·´ç·Šé–‰
            emotion_scores[EmotionType.SAD] += 0.2
        if average_eyebrow > 10.0:  # çœ‰æ¯›ä¸‹å‚
            emotion_scores[EmotionType.SAD] += 0.1

        # ç”Ÿæ°£æª¢æ¸¬
        if nose_wrinkle > 0.12:  # é¼»ç¿¼å¼µé–‹
            emotion_scores[EmotionType.ANGRY] += 0.3
        if average_eyebrow > 15.0:  # çœ‰æ¯›å£“ä½
            emotion_scores[EmotionType.ANGRY] += 0.3
        if mouth_aspect_ratio < 0.03 and mouth_curvature < 0:  # å˜´å·´ç·Šé–‰ä¸”ä¸‹å‚
            emotion_scores[EmotionType.ANGRY] += 0.2
        if eye_openness < 0.2:  # çœ¼ç›ç‡èµ·
            emotion_scores[EmotionType.ANGRY] += 0.2

        # ææ‡¼æª¢æ¸¬
        if eye_openness > 0.35 and mouth_aspect_ratio > 0.08:
            emotion_scores[EmotionType.FEARFUL] += 0.4
        if average_eyebrow < -5.0:  # çœ‰æ¯›ä¸Šæš
            emotion_scores[EmotionType.FEARFUL] += 0.3
        if mouth_width < 0.08:  # å˜´å·´æ”¶ç¸®
            emotion_scores[EmotionType.FEARFUL] += 0.2

        # å­æƒ¡æª¢æ¸¬
        if nose_wrinkle > 0.1 and mouth_curvature < -2.0:
            emotion_scores[EmotionType.DISGUSTED] += 0.4
        if eye_openness < 0.25:  # çœ¼ç›ç‡èµ·
            emotion_scores[EmotionType.DISGUSTED] += 0.3
        if mouth_aspect_ratio < 0.05:  # å˜´å·´ç·Šé–‰
            emotion_scores[EmotionType.DISGUSTED] += 0.2

        # æ‰¾åˆ°æœ€é«˜åˆ†çš„æƒ…ç·’
        detected_emotion = max(emotion_scores, key=emotion_scores.get)
        confidence = emotion_scores[detected_emotion]

        # å¦‚æœæ‰€æœ‰æƒ…ç·’åˆ†æ•¸éƒ½å¾ˆä½ï¼Œè¿”å›ä¸­æ€§
        if confidence < 0.3:
            detected_emotion = EmotionType.NEUTRAL
            confidence = 0.5

        # é™åˆ¶ä¿¡å¿ƒåº¦ç¯„åœ
        confidence = max(0.0, min(1.0, confidence))

        # å„²å­˜åˆ†æ•¸å¿«ç…§ä¾›å¤–éƒ¨æª¢è¦–
        self.latest_scores = {emotion.value: round(score, 4) for emotion, score in emotion_scores.items()}

        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        self.emotion_history.append((detected_emotion, confidence))

        return detected_emotion, confidence

    def get_latest_scores(self) -> Dict[str, float]:
        """å–å¾—æœ€è¿‘ä¸€æ¬¡æƒ…ç·’åˆ†æ•¸åˆ†å¸ƒã€‚"""
        return dict(self.latest_scores)

    def get_emotion_trend(self) -> Dict:
        """ç²å–æƒ…ç·’è¶¨å‹¢åˆ†æ"""
        if not self.emotion_history:
            return {"trend": "stable", "dominant_emotion": EmotionType.NEUTRAL.value}

        # çµ±è¨ˆæœ€è¿‘æƒ…ç·’åˆ†å¸ƒ
        emotion_counts = {}
        total_confidence = 0

        for emotion, confidence in self.emotion_history:
            if emotion not in emotion_counts:
                emotion_counts[emotion] = 0
            emotion_counts[emotion] += confidence
            total_confidence += confidence

        # æ‰¾åˆ°ä¸»å°æƒ…ç·’
        dominant_emotion = max(emotion_counts, key=emotion_counts.get)

        # è¨ˆç®—è¶¨å‹¢ç©©å®šæ€§
        recent_emotions = [e[0] for e in list(self.emotion_history)[-5:]]
        unique_recent = len(set(recent_emotions))

        if unique_recent <= 2:
            trend = "stable"
        elif unique_recent <= 3:
            trend = "changing"
        else:
            trend = "volatile"

        return {
            "trend": trend,
            "dominant_emotion": dominant_emotion.value,
            "confidence_average": total_confidence / len(self.emotion_history) if self.emotion_history else 0.5,
            "emotion_distribution": {e.value: count for e, count in emotion_counts.items()}
        }


class EmotionService:
    """æƒ…ç·’è¾¨è­˜æœå‹™ä¸»é¡"""

    def __init__(self, status_broadcaster: StatusBroadcaster):
        self.status_broadcaster = status_broadcaster
        self.feature_extractor = FacialFeatureExtractor()
        self.emotion_detector = EmotionDetector()

        if not self.feature_extractor.is_available():
            logger.error(
                "MediaPipe FaceMesh æœªå•Ÿç”¨ï¼Œæƒ…ç·’åˆ†æåŠŸèƒ½å°‡ä¸å¯ç”¨: %s",
                self.feature_extractor.init_error,
            )

        # ç°¡åŒ–çš„æœå‹™è¨­è¨ˆï¼šåªè™•ç†åœ–ç‰‡åˆ†æï¼Œä¸ç®¡ç†æ”å½±æ©Ÿæˆ–æª¢æ¸¬ç‹€æ…‹

    # ç§»é™¤äº†æ”å½±æ©Ÿç›¸é—œåŠŸèƒ½ï¼Œä¿æŒæœå‹™ç°¡æ½”å°ˆæ³¨æ–¼åœ–ç‰‡åˆ†æ

    def analyze_image(self, image_path: str) -> Dict:
        """
        åˆ†æå–®å¼µåœ–ç‰‡çš„æƒ…ç·’å…§å®¹ã€‚

        Args:
            image_path (str): åœ–ç‰‡æª”æ¡ˆè·¯å¾„

        Returns:
            Dict: æƒ…ç·’åˆ†æçµæœ
        """
        try:
            if not self.feature_extractor.is_available():
                raise ValueError(self.feature_extractor.init_error or "MediaPipe FaceMesh æœªå°±ç·’")

            start_time = time.time()
            # è®€å–åœ–ç‰‡
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")

            # æå–ç‰¹å¾µ
            features = self.feature_extractor.extract_features(image, static_image=True)
            if not features:
                return {
                    "emotion": EmotionType.NEUTRAL.value,
                    "confidence": 0.3,
                    "message": "æœªæª¢æ¸¬åˆ°è‡‰éƒ¨ç‰¹å¾µ",
                    "analysis_time": _now_ts(),
                    "processing_time": round(time.time() - start_time, 3),
                }

            # æª¢æ¸¬æƒ…ç·’
            emotion, confidence = self.emotion_detector.detect_emotion(features)

            return {
                "emotion": emotion.value,
                "confidence": round(confidence, 3),
                "features": {k: round(float(v), 4) for k, v in features.items()},
                "score_breakdown": self.emotion_detector.get_latest_scores(),
                "message": f"æª¢æ¸¬åˆ°æƒ…ç·’: {emotion.value}",
                "analysis_time": _now_ts(),
                "processing_time": round(time.time() - start_time, 3),
            }

        except Exception as exc:
            return {
                "emotion": EmotionType.NEUTRAL.value,
                "confidence": 0.0,
                "message": f"åˆ†æå¤±æ•—: {str(exc)}",
                "error": str(exc),
                "analysis_time": _now_ts()
            }

    def analyze_video(self, video_path: str) -> Dict:
        """
        åˆ†æå½±ç‰‡æª”æ¡ˆçš„æƒ…ç·’å…§å®¹ã€‚

        Args:
            video_path (str): å½±ç‰‡æª”æ¡ˆè·¯å¾„

        Returns:
            Dict: æƒ…ç·’åˆ†æçµæœ
        """
        try:
            if not self.feature_extractor.is_available():
                raise ValueError(self.feature_extractor.init_error or "MediaPipe FaceMesh æœªå°±ç·’")

            start_time = time.time()
            # é–‹å•Ÿå½±ç‰‡æª”æ¡ˆ
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")

            # ç²å–å½±ç‰‡è³‡è¨Š
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0

            # é‡ç½®æª¢æ¸¬å™¨æ­·å²
            self.emotion_detector.emotion_history.clear()

            emotions_detected = []
            frames_processed = 0
            feature_sums: Dict[str, float] = {}
            sample_interval = max(1, fps // 2)  # æ¯ç§’å–2å¹€åˆ†æ

            frame_idx = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # æŒ‰é–“éš”æ¡æ¨£å¹€
                if frame_idx % sample_interval == 0:
                    # æå–ç‰¹å¾µ
                    features = self.feature_extractor.extract_features(frame)

                    if features:
                        # æª¢æ¸¬æƒ…ç·’
                        emotion, confidence = self.emotion_detector.detect_emotion(features)

                        for key, value in features.items():
                            feature_sums[key] = feature_sums.get(key, 0.0) + float(value)

                        emotions_detected.append({
                            "timestamp": frame_idx / fps if fps > 0 else 0,
                            "emotion": emotion.value,
                            "confidence": round(confidence, 3),
                            "scores": self.emotion_detector.get_latest_scores(),
                        })
                        frames_processed += 1

                frame_idx += 1

            cap.release()

            if not emotions_detected:
                return {
                    "dominant_emotion": EmotionType.NEUTRAL.value,
                    "confidence_average": 0.3,
                    "emotions_timeline": [],
                    "message": "å½±ç‰‡ä¸­æœªæª¢æ¸¬åˆ°è‡‰éƒ¨ç‰¹å¾µ",
                    "video_info": {
                        "duration": duration,
                        "fps": fps,
                        "frame_count": frame_count,
                        "frames_processed": frames_processed
                    },
                    "feature_averages": {},
                    "analysis_time": _now_ts(),
                    "processing_time": round(time.time() - start_time, 3),
                }

            # è¨ˆç®—çµ±è¨ˆè³‡æ–™
            emotion_counts = {}
            total_confidence = 0

            for detection in emotions_detected:
                emotion_name = detection["emotion"]
                confidence = detection["confidence"]

                if emotion_name not in emotion_counts:
                    emotion_counts[emotion_name] = {"count": 0, "total_confidence": 0}

                emotion_counts[emotion_name]["count"] += 1
                emotion_counts[emotion_name]["total_confidence"] += confidence
                total_confidence += confidence

            # æ‰¾åˆ°ä¸»å°æƒ…ç·’
            dominant_emotion = max(emotion_counts.keys(),
                                 key=lambda x: emotion_counts[x]["total_confidence"])

            # è¨ˆç®—æƒ…ç·’åˆ†å¸ƒç™¾åˆ†æ¯”
            emotion_distribution = {}
            for emotion_name, data in emotion_counts.items():
                percentage = (data["count"] / len(emotions_detected)) * 100
                avg_confidence = data["total_confidence"] / data["count"]
                emotion_distribution[emotion_name] = {
                    "percentage": round(percentage, 2),
                    "average_confidence": round(avg_confidence, 3)
                }

            feature_averages = {
                feature: round(value / frames_processed, 4)
                for feature, value in feature_sums.items()
                if frames_processed
            }

            return {
                "dominant_emotion": dominant_emotion,
                "confidence_average": round(total_confidence / len(emotions_detected), 3),
                "emotion_distribution": emotion_distribution,
                "emotions_timeline": self._get_key_moments(emotions_detected),
                "trend_analysis": self.emotion_detector.get_emotion_trend(),
                "message": f"å½±ç‰‡åˆ†æå®Œæˆï¼Œä¸»è¦æƒ…ç·’: {dominant_emotion}",
                "video_info": {
                    "duration": duration,
                    "fps": fps,
                    "frame_count": frame_count,
                    "frames_processed": frames_processed,
                    "sample_interval": sample_interval
                },
                "feature_averages": feature_averages,
                "analysis_time": _now_ts(),
                "processing_time": round(time.time() - start_time, 3),
            }

        except Exception as exc:
            return {
                "dominant_emotion": EmotionType.NEUTRAL.value,
                "confidence_average": 0.0,
                "message": f"å½±ç‰‡åˆ†æå¤±æ•—: {str(exc)}",
                "error": str(exc),
                "analysis_time": _now_ts()
            }

    def _get_key_moments(self, emotions_detected: List[Dict]) -> List[Dict]:
        """
        å¾å®Œæ•´çš„æƒ…ç·’æª¢æ¸¬çµæœä¸­ç¯©é¸å‡ºé—œéµæ™‚åˆ»ï¼Œé¿å…å†—é•·çš„æ™‚é–“è»¸ã€‚

        Args:
            emotions_detected: å®Œæ•´çš„æƒ…ç·’æª¢æ¸¬çµæœåˆ—è¡¨

        Returns:
            ç²¾ç°¡çš„é—œéµæ™‚åˆ»åˆ—è¡¨
        """
        if not emotions_detected:
            return []

        key_moments = []
        last_emotion = None
        last_added_time = -10  # ç¢ºä¿ç¬¬ä¸€å€‹èƒ½è¢«åŠ å…¥

        for detection in emotions_detected:
            current_emotion = detection["emotion"]
            current_time = detection["timestamp"]

            # æ¢ä»¶1: æƒ…ç·’è®ŠåŒ–
            emotion_changed = current_emotion != last_emotion

            # æ¢ä»¶2: æ™‚é–“é–“éš”è¶³å¤  (è‡³å°‘5ç§’)
            time_gap_enough = current_time - last_added_time >= 5.0

            # æ¢ä»¶3: é«˜ä¿¡å¿ƒåº¦æª¢æ¸¬ (>80%)
            high_confidence = detection["confidence"] > 0.8

            # æ·»åŠ é—œéµæ™‚åˆ»çš„æ¢ä»¶
            should_add = (
                emotion_changed or  # æƒ…ç·’è®ŠåŒ–ç¸½æ˜¯é‡è¦
                (time_gap_enough and high_confidence)  # æˆ–è€…é–“éš”å¤ é•·ä¸”ä¿¡å¿ƒåº¦é«˜
            )

            if should_add:
                key_moments.append(detection)
                last_added_time = current_time
                last_emotion = current_emotion

        # ç¢ºä¿è‡³å°‘æœ‰é–‹å§‹å’ŒçµæŸæ™‚åˆ»
        if len(key_moments) == 0 and emotions_detected:
            # å¦‚æœæ²’æœ‰é—œéµæ™‚åˆ»ï¼Œè‡³å°‘åŒ…å«ç¬¬ä¸€å€‹å’Œæœ€å¾Œä¸€å€‹
            key_moments = [emotions_detected[0]]
            if len(emotions_detected) > 1:
                key_moments.append(emotions_detected[-1])
        elif len(key_moments) == 1 and len(emotions_detected) > 1:
            # å¦‚æœåªæœ‰ä¸€å€‹é—œéµæ™‚åˆ»ï¼ŒåŠ ä¸Šæœ€å¾Œä¸€å€‹
            if key_moments[0] != emotions_detected[-1]:
                key_moments.append(emotions_detected[-1])

        # é™åˆ¶æœ€å¤šé¡¯ç¤º10å€‹é—œéµæ™‚åˆ»
        return key_moments[:10]


    def _create_simple_result(self, emotion: str, confidence: float) -> Dict:
        """
        å‰µå»ºç°¡åŒ–çš„æƒ…ç·’åˆ†æçµæœï¼ŒåªåŒ…å«æ ¸å¿ƒä¿¡æ¯

        Args:
            emotion: æƒ…ç·’åç¨± (ä¸­æ–‡)
            confidence: ä¿¡å¿ƒåº¦

        Returns:
            ç°¡åŒ–çš„çµæœå­—å…¸
        """
        emotion_info = EMOTION_TRANSLATIONS.get(emotion, {
            "en": "unknown",
            "zh": emotion,
            "emoji": "â“"
        })

        return {
            "emotion_zh": emotion_info["zh"],
            "emotion_en": emotion_info["en"],
            "emoji": emotion_info["emoji"],
            "confidence": round(confidence, 3)
        }

    def analyze_image_simple(self, image_path: str) -> Dict:
        """
        ç°¡åŒ–ç‰ˆåœ–ç‰‡æƒ…ç·’åˆ†æï¼Œåªè¿”å›æ ¸å¿ƒçµæœ

        Args:
            image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘

        Returns:
            ç°¡åŒ–çš„åˆ†æçµæœ
        """
        try:
            if not self.feature_extractor.is_available():
                raise ValueError(self.feature_extractor.init_error or "MediaPipe FaceMesh æœªå°±ç·’")

            # è®€å–åœ–ç‰‡
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")

            # æå–ç‰¹å¾µ
            features = self.feature_extractor.extract_features(image, static_image=True)
            if not features:
                return self._create_simple_result("ä¸­æ€§", 0.3)

            # æª¢æ¸¬æƒ…ç·’
            emotion, confidence = self.emotion_detector.detect_emotion(features)

            return self._create_simple_result(emotion.value, confidence)

        except Exception as exc:
            return {
                "emotion_zh": "ä¸­æ€§",
                "emotion_en": "neutral",
                "emoji": "ğŸ˜",
                "confidence": 0.0,
                "error": str(exc)
            }

    def analyze_video_simple(self, video_path: str) -> Dict:
        """
        ç°¡åŒ–ç‰ˆå½±ç‰‡æƒ…ç·’åˆ†æï¼Œåªè¿”å›ä¸»è¦æƒ…ç·’çµæœ

        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘

        Returns:
            ç°¡åŒ–çš„åˆ†æçµæœ
        """
        try:
            # å…ˆç”¨å®Œæ•´åˆ†æç²å–çµæœ
            full_result = self.analyze_video(video_path)

            if "error" in full_result:
                return {
                    "emotion_zh": "ä¸­æ€§",
                    "emotion_en": "neutral",
                    "emoji": "ğŸ˜",
                    "confidence": 0.0,
                    "error": full_result.get("error", "Unknown error")
                }

            # æå–ä¸»è¦æƒ…ç·’
            dominant_emotion = full_result.get("dominant_emotion", "ä¸­æ€§")
            confidence = full_result.get("confidence_average", 0.0)

            return self._create_simple_result(dominant_emotion, confidence)

        except Exception as exc:
            return {
                "emotion_zh": "ä¸­æ€§",
                "emotion_en": "neutral",
                "emoji": "ğŸ˜",
                "confidence": 0.0,
                "error": str(exc)
            }

    def analyze_video_deepface_stream(self, video_path: str, frame_interval: float = 0.5):
        """
        ä½¿ç”¨ DeepFace é€²è¡Œå½±ç‰‡ä¸²æµæƒ…ç·’åˆ†æ (é€å¹€æˆªå–åˆ†æ)

        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
            frame_interval: æˆªå¹€é–“éš”(ç§’), é»˜èª0.5ç§’

        Yields:
            Dict: æ¯ä¸€å¹€çš„æƒ…ç·’åˆ†æçµæœ
        """
        if not _DEEPFACE_AVAILABLE:
            yield {
                "error": f"DeepFace ä¸å¯ç”¨: {_DEEPFACE_ERROR}",
                "frame_time": 0,
                "completed": True
            }
            return

        try:
            import tempfile
            import os

            # é–‹å•Ÿå½±ç‰‡æª”æ¡ˆ
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                yield {
                    "error": f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}",
                    "frame_time": 0,
                    "completed": True
                }
                return

            # ç²å–å½±ç‰‡è³‡è¨Š
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0
            frame_skip = max(1, int(fps * frame_interval))  # è¦è·³éçš„å¹€æ•¸

            logger.info(f"é–‹å§‹DeepFaceå½±ç‰‡åˆ†æ: FPS={fps}, ç¸½å¹€æ•¸={total_frames}, é–“éš”={frame_interval}ç§’")

            frame_count = 0
            analyzed_count = 0

            # å‰µå»ºè‡¨æ™‚ç›®éŒ„ä¾†å­˜æ”¾æˆªåœ–
            with tempfile.TemporaryDirectory() as temp_dir:
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # æŒ‰é–“éš”è™•ç†å¹€
                    if frame_count % frame_skip == 0:
                        current_time = frame_count / fps if fps > 0 else 0

                        # ä¿å­˜ç•¶å‰å¹€ç‚ºè‡¨æ™‚åœ–ç‰‡
                        temp_image_path = os.path.join(temp_dir, f"frame_{analyzed_count:06d}.jpg")
                        cv2.imwrite(temp_image_path, frame)

                        # ä½¿ç”¨DeepFaceåˆ†æé€™ä¸€å¹€
                        analysis_result = self.analyze_image_deepface(temp_image_path)

                        # æ·»åŠ æ™‚é–“æˆ³å’Œé€²åº¦ä¿¡æ¯
                        analysis_result.update({
                            "frame_time": round(current_time, 2),
                            "frame_number": frame_count,
                            "analyzed_frame": analyzed_count,
                            "progress": round((frame_count / total_frames) * 100, 1) if total_frames > 0 else 0,
                            "total_duration": round(duration, 2),
                            "completed": False
                        })

                        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                        try:
                            os.unlink(temp_image_path)
                        except:
                            pass

                        yield analysis_result
                        analyzed_count += 1

                    frame_count += 1

                    # é˜²æ­¢è¨˜æ†¶é«”éè¼‰ï¼Œé™åˆ¶åˆ†æå¹€æ•¸
                    if analyzed_count >= 1200:  # æœ€å¤š10åˆ†é˜ (0.5ç§’é–“éš”)
                        logger.warning("é”åˆ°åˆ†æå¹€æ•¸é™åˆ¶ï¼Œåœæ­¢åˆ†æ")
                        break

            cap.release()

            # ç™¼é€å®Œæˆä¿¡è™Ÿ
            yield {
                "message": f"å½±ç‰‡åˆ†æå®Œæˆï¼Œå…±åˆ†æäº† {analyzed_count} å¹€",
                "total_frames": total_frames,
                "analyzed_frames": analyzed_count,
                "total_duration": round(duration, 2),
                "frame_time": round(duration, 2),
                "completed": True
            }

        except Exception as exc:
            logger.error(f"DeepFace å½±ç‰‡åˆ†æå¤±æ•—: {exc}")
            yield {
                "error": f"å½±ç‰‡åˆ†æéŒ¯èª¤: {str(exc)}",
                "frame_time": 0,
                "completed": True
            }

    def analyze_image_deepface(self, image_path: str) -> Dict:
        """
        ä½¿ç”¨ DeepFace é€²è¡Œäººè‡‰ç‰¹å¾µåˆ†æå’Œæƒ…ç·’æ¨æ¸¬

        Args:
            image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘

        Returns:
            DeepFace åˆ†æçµæœ
        """

        if not _DEEPFACE_AVAILABLE and DeepFace is None:
            return {
                "emotion_zh": "ä¸­æ€§",
                "emotion_en": "neutral",
                "emoji": "ğŸ˜",
                "confidence": 0.0,
                "error": f"DeepFace ä¸å¯ç”¨: {_DEEPFACE_ERROR}"
            }

        try:
            # å°å…¥ TensorFlow ç”¨æ–¼è¨˜æ†¶é«”ç®¡ç†
            import tensorflow as tf

            analyze_kwargs = dict(
                img_path=image_path,
                actions=['emotion'],
                enforce_detection=False,  # æ›´å¯¬é¬†çš„äººè‡‰æª¢æ¸¬
                detector_backend='opencv',  # ä½¿ç”¨ GPU å‹å¥½çš„ detector
            )

            if _GPU_STATUS.tensorflow_ready:
                with tf.device('/GPU:0'):
                    analysis = DeepFace.analyze(**analyze_kwargs)
            else:
                analysis = DeepFace.analyze(**analyze_kwargs)

            # DeepFace è¿”å›ä¸€å€‹åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æ˜¯ä¸€å¼µè‡‰çš„åˆ†æçµæœ
            if not analysis or not isinstance(analysis, list) or len(analysis) == 0:
                return {
                    "emotion_zh": "æœªæª¢æ¸¬åˆ°",
                    "emotion_en": "not_detected",
                    "emoji": "â“",
                    "confidence": 0.0,
                    "error": "æœªæª¢æ¸¬åˆ°äººè‡‰",
                    "engine": "deepface",
                    "face_detected": False
                }

            # æˆ‘å€‘åªå–ç¬¬ä¸€å¼µè‡‰çš„çµæœ
            result = analysis[0]

            # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è‡‰éƒ¨æª¢æ¸¬çµæœ
            if 'dominant_emotion' not in result or 'emotion' not in result:
                return {
                    "emotion_zh": "æœªæª¢æ¸¬åˆ°",
                    "emotion_en": "not_detected",
                    "emoji": "â“",
                    "confidence": 0.0,
                    "error": "è‡‰éƒ¨æª¢æ¸¬å¤±æ•—",
                    "engine": "deepface",
                    "face_detected": False
                }

            dominant_emotion_en = result['dominant_emotion']
            confidence = result['emotion'][dominant_emotion_en] / 100.0

            # å¦‚æœæ‰€æœ‰æƒ…ç·’çš„ä¿¡å¿ƒåº¦éƒ½å¾ˆä½ï¼ˆéƒ½æ¥è¿‘0ï¼‰ï¼Œè¡¨ç¤ºå¯¦éš›ä¸Šæ²’æœ‰æª¢æ¸¬åˆ°è‡‰
            all_emotions_low = all(score <= 1.0 for score in result['emotion'].values())  # 1%ä»¥ä¸‹ç®—ä½œæœªæª¢æ¸¬
            if confidence <= 0.01 or all_emotions_low:  # ä¿¡å¿ƒåº¦å°æ–¼1%æˆ–æ‰€æœ‰æƒ…ç·’éƒ½å¾ˆä½
                return {
                    "emotion_zh": "æœªæª¢æ¸¬åˆ°",
                    "emotion_en": "not_detected",
                    "emoji": "â“",
                    "confidence": 0.0,
                    "error": "æœªæª¢æ¸¬åˆ°æœ‰æ•ˆçš„äººè‡‰ç‰¹å¾µ",
                    "engine": "deepface",
                    "face_detected": False
                }

            # è‹±æ–‡è½‰ä¸­æ–‡
            emotion_zh = "é¢ç„¡è¡¨æƒ…" # é è¨­å€¼
            for zh, details in EMOTION_TRANSLATIONS.items():
                if details['en'] == dominant_emotion_en:
                    emotion_zh = zh
                    break

            emoji = EMOTION_TRANSLATIONS.get(emotion_zh, {}).get("emoji", "ğŸ˜")

            # å–å¾—å…¶ä»–ç‰¹å¾µåˆ†æçµæœ
            age = result.get('age', 0)

            # æ€§åˆ¥åˆ†æ
            gender_analysis = result.get('gender', {})
            if isinstance(gender_analysis, dict) and gender_analysis:
                gender_scores = {k.lower(): v for k, v in gender_analysis.items()}
                dominant_gender_key = max(gender_scores, key=gender_scores.get)
                gender_zh = 'ç”·æ€§' if dominant_gender_key == 'man' else 'å¥³æ€§' if dominant_gender_key == 'woman' else 'æœªçŸ¥'
                gender_confidence = gender_scores.get(dominant_gender_key, 0) / 100.0
            else:
                gender_zh = 'æœªçŸ¥'
                dominant_gender_key = 'unknown'
                gender_confidence = 0.0
                gender_scores = {}

            # ç¨®æ—åˆ†æ
            race_analysis = result.get('race', {})
            if isinstance(race_analysis, dict) and race_analysis:
                race_scores = {k.lower().replace(' ', '_'): v for k, v in race_analysis.items()}
                dominant_race_key = max(race_scores, key=race_scores.get)

                # ç¨®æ—ä¸­æ–‡æ˜ å°„
                race_mapping = {
                    'asian': 'äºæ´²äºº',
                    'white': 'ç™½äºº',
                    'black': 'é»‘äºº',
                    'indian': 'å°åº¦äºº',
                    'latino_hispanic': 'æ‹‰ä¸è£”',
                    'middle_eastern': 'ä¸­æ±äºº'
                }
                race_zh = race_mapping.get(dominant_race_key, 'æœªçŸ¥')
                race_confidence = race_scores.get(dominant_race_key, 0) / 100.0
            else:
                race_zh = 'æœªçŸ¥'
                dominant_race_key = 'unknown'
                race_confidence = 0.0
                race_scores = {}

            # åˆ†æå®Œæˆå¾Œæ¸…ç† TensorFlow sessionï¼ˆé˜²æ­¢è¨˜æ†¶é«”ç´¯ç©ï¼‰
            try:
                tf.keras.backend.clear_session()
            except:
                pass  # å¦‚æœæ¸…ç†å¤±æ•—ä¹Ÿä¸å½±éŸ¿çµæœ

            return {
                "emotion_zh": emotion_zh,
                "emotion_en": dominant_emotion_en,
                "emoji": emoji,
                "confidence": round(confidence, 3),
                "engine": "deepface",
                "face_detected": True,
                "raw_scores": {k: round(v / 100.0, 4) for k, v in result['emotion'].items()}
            }

        except Exception as exc:
            logger.error(f"DeepFace åˆ†æå¤±æ•—: {exc}")

            # éŒ¯èª¤æ™‚ä¹Ÿæ¸…ç† sessionï¼ˆé˜²æ­¢è¨˜æ†¶é«”æ´©æ¼ï¼‰
            try:
                import tensorflow as tf
                tf.keras.backend.clear_session()
            except:
                pass

            return {
                "emotion_zh": "é¢ç„¡è¡¨æƒ…",
                "emotion_en": "neutral",
                "emoji": "ğŸ˜",
                "confidence": 0.0,
                "error": f"DeepFace åˆ†æéŒ¯èª¤: {str(exc)}",
                "engine": "deepface"
            }
