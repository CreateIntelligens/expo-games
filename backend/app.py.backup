# =============================================================================
# backend/app.py - FastAPI Application for AI Interactive Games
# =============================================================================
# This module implements a comprehensive FastAPI web application that provides
# multiple AI-powered interactive experiences including emotion analysis, action
# detection, hand gesture recognition, rock-paper-scissors games, and AI drawing
# recognition. It includes REST API endpoints for controlling various services
# and WebSocket endpoints for real-time updates across all interactive modules.
# The application uses computer vision and machine learning services to create
# engaging interactive experiences for exhibitions and entertainment venues.
# =============================================================================

import asyncio
import base64
import json
import os
import tempfile
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Optional

from fastapi import FastAPI, File, Form, HTTPException, Request, UploadFile, WebSocket
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from starlette.middleware.cors import CORSMiddleware
from starlette.websockets import WebSocketDisconnect

from .config.settings import APP_TITLE, CORS_ALLOW_ORIGINS, MAX_UPLOAD_SIZE_BYTES
from .services.emotion_service import EmotionService
from .services.action_detection_service import ActionDetectionService
from .services.hand_gesture_service import HandGestureService
from .services.rps_game_service import RPSGameService
from .services.drawing_service import DrawingService
from .services.status_broadcaster import StatusBroadcaster
from .utils.gpu_runtime import get_gpu_status_dict


# Project directory structure setup
BASE_DIR = Path(__file__).resolve().parent.parent
FRONTEND_DIR = BASE_DIR / "frontend"
TEMPLATES_DIR = FRONTEND_DIR / "templates"
STATIC_DIR = FRONTEND_DIR / "static"

# Initialize core services with shared status broadcaster
status_broadcaster = StatusBroadcaster()
emotion_service = EmotionService(status_broadcaster)
action_service = ActionDetectionService(status_broadcaster)
hand_gesture_service = HandGestureService(status_broadcaster)
rps_game_service = RPSGameService(status_broadcaster)  # MediaPipe æ‰‹å‹¢è¾¨è­˜ç‰ˆæœ¬
drawing_service = DrawingService(status_broadcaster)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan context manager for FastAPI.

    Handles application startup and shutdown events. During startup, it sets
    the asyncio event loop for the status broadcaster service to ensure proper
    async operations throughout the application lifecycle.

    Args:
        app (FastAPI): The FastAPI application instance.

    Yields:
        None: Control is yielded back to FastAPI after startup setup.

    Note:
        This replaces the deprecated @app.on_event("startup") decorator
        with the modern lifespan event handler.
    """
    loop = asyncio.get_running_loop()
    status_broadcaster.set_loop(loop)
    yield


app = FastAPI(title=APP_TITLE, description="Standalone emotion analysis service",
              version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ALLOW_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")
templates = Jinja2Templates(directory=str(TEMPLATES_DIR))


@app.get("/", response_class=HTMLResponse)
async def render_emotion_page(request: Request) -> HTMLResponse:
    """
    Render the main emotion and action detection page.

    Serves the HTML template for the web interface that allows users to interact
    with emotion analysis and action detection features through both manual controls
    and real-time WebSocket updates.

    Args:
        request (Request): The incoming HTTP request object.

    Returns:
        HTMLResponse: Rendered HTML page with application title and file size limits.

    Example:
        >>> response = await render_emotion_page(request)
        >>> response.status_code
        200
    """
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "title": APP_TITLE,
            "max_file_size_mb": MAX_UPLOAD_SIZE_BYTES // (1024 * 1024),
        },
    )

@app.get("/docs/ws", response_class=HTMLResponse)
async def render_websocket_docs(request: Request) -> HTMLResponse:
    """
    Render the WebSocket API documentation page.

    Provides an interactive documentation interface for all WebSocket endpoints
    with real-time testing capabilities and detailed message format specifications.

    Args:
        request (Request): The incoming HTTP request object.

    Returns:
        HTMLResponse: Interactive WebSocket API documentation page.

    Example:
        >>> response = await render_websocket_docs(request)
        >>> response.status_code
        200
    """
    return templates.TemplateResponse(
        "websocket-docs.html",
        {
            "request": request,
            "title": f"{APP_TITLE} - WebSocket API æ–‡æª”",
        },
    )


@app.get("/api/system/gpu")
async def gpu_status() -> dict:
    """Return TensorFlow / MediaPipe GPU availability details."""

    return get_gpu_status_dict()











@app.post("/api/emotion/analyze/image")
async def api_analyze_emotion_image(file: UploadFile = File(...)) -> JSONResponse:
    """
    åœ–ç‰‡æƒ…ç·’åˆ†æ - ä½¿ç”¨ DeepFace é€²è¡Œåœ–ç‰‡æƒ…ç·’æª¢æ¸¬

    åˆ†æä¸Šå‚³çš„åœ–ç‰‡æª”æ¡ˆï¼Œæª¢æ¸¬äººè‡‰æƒ…ç·’ä¸¦è¿”å›åˆ†æçµæœã€‚

    Args:
        file (UploadFile): ä¸Šå‚³çš„åœ–ç‰‡æª”æ¡ˆ

    Returns:
        JSONResponse: DeepFace æƒ…ç·’åˆ†æçµæœï¼ŒåŒ…å«ä¸­æ–‡/è‹±æ–‡åç¨±å’Œä¿¡å¿ƒåº¦

    Example:
        >>> response = await api_analyze_emotion_image(image_file)
        >>> response.json()
        {
            'emotion_zh': 'é–‹å¿ƒ',
            'emotion_en': 'happy',
            'emoji': 'ğŸ˜Š',
            'confidence': 0.92
        }
    """
    # Validate file presence
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›æª”æ¡ˆ")

    # Extract and validate file extension
    file_ext = os.path.splitext(file.filename.lower())[1]
    image_exts = {".jpg", ".jpeg", ".png", ".bmp", ".gif"}

    if file_ext not in image_exts:
        raise HTTPException(
            status_code=400, detail=f"DeepFace åƒ…æ”¯æ´åœ–ç‰‡æ ¼å¼ï¼Œæ”¶åˆ°: {file_ext}")

    # Read and validate file size
    file_content = await file.read()
    if len(file_content) > MAX_UPLOAD_SIZE_BYTES:
        limit_mb = MAX_UPLOAD_SIZE_BYTES // (1024 * 1024)
        raise HTTPException(status_code=413, detail=f"æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± {limit_mb}MB")

    # Create temporary file for processing
    suffix = file_ext or ""
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(file_content)
        temp_path = tmp.name

    try:
        # Use local DeepFace analysis
        result = emotion_service.analyze_image_deepface(temp_path)
        return JSONResponse(result)

    except Exception as e:
        return JSONResponse({
            'emotion_zh': 'ä¸­æ€§',
            'emotion_en': 'neutral',
            'emoji': 'ğŸ˜',
            'confidence': 0.0,
            'error': f"DeepFace åˆ†æéŒ¯èª¤: {str(e)}"
        })
    finally:
        # Cleanup temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)





@app.post("/api/emotion/analyze/video")
async def api_analyze_emotion_video(
    file: UploadFile = File(...),
    frame_interval: float = Form(0.5)
) -> StreamingResponse:
    """
    å½±ç‰‡æƒ…ç·’åˆ†æ - ä½¿ç”¨ DeepFace é€²è¡Œå½±ç‰‡æƒ…ç·’æª¢æ¸¬

    é€å¹€æˆªå–å½±ç‰‡ä¸¦ä½¿ç”¨DeepFaceé€²è¡Œæƒ…ç·’åˆ†æï¼Œä»¥Server-Sent Eventsä¸²æµè¿”å›çµæœã€‚

    Args:
        file (UploadFile): ä¸Šå‚³çš„å½±ç‰‡æª”æ¡ˆ
        frame_interval (float): æˆªå¹€é–“éš”(ç§’)ï¼Œé»˜èª0.5ç§’

    Returns:
        StreamingResponse: SSEæ ¼å¼çš„ä¸²æµåˆ†æçµæœ
    """
    # é©—è­‰æª”æ¡ˆ
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›æª”æ¡ˆ")

    # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
    file_ext = os.path.splitext(file.filename.lower())[1]
    video_exts = {".mp4", ".avi", ".mov", ".mkv", ".flv", ".wmv", ".webm"}

    if file_ext not in video_exts:
        raise HTTPException(status_code=400, detail=f"åƒ…æ”¯æ´å½±ç‰‡æ ¼å¼ï¼Œæ”¶åˆ°: {file_ext}")

    # æª¢æŸ¥æª”æ¡ˆå¤§å°
    file_content = await file.read()
    if len(file_content) > MAX_UPLOAD_SIZE_BYTES:
        limit_mb = MAX_UPLOAD_SIZE_BYTES // (1024 * 1024)
        raise HTTPException(status_code=413, detail=f"æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± {limit_mb}MB")

    # é©—è­‰æˆªå¹€é–“éš”
    if frame_interval < 0.1 or frame_interval > 5.0:
        raise HTTPException(status_code=400, detail="æˆªå¹€é–“éš”å¿…é ˆåœ¨0.1-5.0ç§’ä¹‹é–“")

    # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
    suffix = file_ext or ""
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(file_content)
        temp_path = tmp.name

    def generate_stream():
        """ç”¢ç”ŸSSEæ ¼å¼çš„ä¸²æµæ•¸æ“š"""
        try:
            for result in emotion_service.analyze_video_deepface_stream(temp_path, frame_interval):
                # æ ¼å¼åŒ–ç‚ºSSEæ ¼å¼
                data = json.dumps(result, ensure_ascii=False)
                yield f"data: {data}\n\n"

                # å¦‚æœå®Œæˆå‰‡çµæŸ
                if result.get("completed", False):
                    break

        except Exception as e:
            # ç™¼é€éŒ¯èª¤ä¿¡æ¯
            error_data = {
                "error": f"ä¸²æµåˆ†æéŒ¯èª¤: {str(e)}",
                "frame_time": 0,
                "completed": True
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        finally:
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )


@app.post("/api/action/start")
async def api_start_action_detection(difficulty: str = Form("easy")) -> JSONResponse:
    """
    Start action detection process.

    Initiates real-time human action detection with configurable difficulty levels.
    Supports different detection sensitivities for various use cases.

    Args:
        difficulty (str): Detection difficulty level ("easy", "medium", "hard").

    Returns:
        JSONResponse: Status response with success/error information.

    Raises:
        HTTPException: If action detection service fails to start.

    Example:
        >>> response = await api_start_action_detection("medium")
        >>> response.json()
        {'status': 'success', 'message': 'Action detection started'}
    """
    result = action_service.start_action_detection(difficulty)
    if result.get("status") == "error":
        raise HTTPException(
            status_code=400, detail=result.get("message", "å•Ÿå‹•å‹•ä½œæª¢æ¸¬å¤±æ•—"))
    return JSONResponse(result)


@app.post("/api/action/stop")
async def api_stop_action_detection() -> JSONResponse:
    """
    Stop action detection process.

    Terminates the currently running action detection process and releases
    associated resources.

    Returns:
        JSONResponse: Status response indicating successful stop or error details.

    Example:
        >>> response = await api_stop_action_detection()
        >>> response.json()
        {'status': 'success', 'message': 'Action detection stopped'}
    """
    result = action_service.stop_action_detection()
    return JSONResponse(result)


@app.get("/api/action/status")
async def api_action_status() -> JSONResponse:
    """
    Get current action detection status.

    Retrieves the current state of the action detection service, including
    whether it's running, difficulty level, and detection statistics.

    Returns:
        JSONResponse: Current status information of action detection service.

    Example:
        >>> response = await api_action_status()
        >>> response.json()
        {'status': 'running', 'difficulty': 'easy', 'detections': 8}
    """
    return JSONResponse(action_service.get_detection_status())


@app.post("/api/action/analyze")
async def api_analyze_action_video(file: UploadFile = File(...)) -> JSONResponse:
    """
    Analyze action from uploaded video file.

    Processes uploaded video files to detect and analyze actions using computer vision.
    Supports various video formats with automatic type detection and validation.

    Args:
        file (UploadFile): The uploaded video file.

    Returns:
        JSONResponse: Analysis results with action detection data and file information.

    Raises:
        HTTPException: For invalid files, unsupported formats, or size limits exceeded.

    Example:
        >>> response = await api_analyze_action_video(video_file)
        >>> response.json()
        {
            'status': 'success',
            'results': {'primary_action': 'smile', 'confidence': 0.85},
            'file_info': {'name': 'video.mp4', 'type': 'video', 'size': 5242880}
        }
    """
    # Validate file presence
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›æª”æ¡ˆ")

    # Extract and validate file extension
    file_ext = os.path.splitext(file.filename.lower())[1]
    video_exts = {".mp4", ".avi", ".mov", ".mkv", ".wmv", ".webm"}

    if file_ext not in video_exts:
        raise HTTPException(
            status_code=400, detail=f"ä¸æ”¯æ´çš„å½±ç‰‡æ ¼å¼: {file_ext}ï¼Œè«‹ä½¿ç”¨ MP4, AVI, MOV, MKV, WMV, WEBM")

    # Read and validate file size
    file_content = await file.read()
    if len(file_content) > MAX_UPLOAD_SIZE_BYTES:
        limit_mb = MAX_UPLOAD_SIZE_BYTES // (1024 * 1024)
        raise HTTPException(status_code=413, detail=f"æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± {limit_mb}MB")

    # Create temporary file for processing
    suffix = file_ext or ""
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(file_content)
        temp_path = tmp.name

    try:
        # Analyze video
        result = action_service.analyze_video(temp_path)

        # Return comprehensive analysis results
        return JSONResponse({
            "status": "success",
            "message": "å‹•ä½œåˆ†æå®Œæˆ",
            "results": result,
            "file_info": {
                "name": file.filename,
                "type": "video",
                "size": len(file_content),
            },
        })
    finally:
        # Ensure temporary file cleanup
        os.unlink(temp_path)


# =============================================================================
# çŸ³é ­å‰ªåˆ€å¸ƒéŠæˆ² API è·¯ç”±
# =============================================================================

@app.post("/api/rps/start")
async def api_start_rps_game(
    request: Request
) -> JSONResponse:
    """
    Start rock-paper-scissors game session (MediaPipe version).

    Initiates an interactive rock-paper-scissors game using MediaPipe hand gesture recognition.
    Players upload images of their hand gestures, and the system detects rock/paper/scissors
    with high accuracy.

    Args:
        request (Request): Request containing target_score in JSON body.

    Returns:
        JSONResponse: Game initialization status with session details.

    Raises:
        HTTPException: If game service fails to start or invalid parameters provided.

    Example:
        >>> response = await api_start_rps_game(request)
        >>> response.json()
        {'status': 'success', 'target_score': 5}
    """
    body = await request.json()
    target_score = body.get("target_score", 3)

    result = rps_game_service.start_game(target_score)
    if result.get("status") == "error":
        raise HTTPException(
            status_code=400, detail=result.get("message", "å•Ÿå‹•éŠæˆ²å¤±æ•—"))
    return JSONResponse(result)


@app.post("/api/rps/stop")
async def api_stop_rps_game() -> JSONResponse:
    """
    Stop rock-paper-scissors game session.

    Terminates the current game session and cleans up associated resources.
    Saves final scores and game statistics before shutdown.

    Returns:
        JSONResponse: Game termination status with final results.

    Example:
        >>> response = await api_stop_rps_game()
        >>> response.json()
        {'status': 'success', 'final_score': {'player': 3, 'ai': 2}}
    """
    result = rps_game_service.stop_game()
    return JSONResponse(result)


@app.get("/api/rps/status")
async def api_rps_game_status() -> JSONResponse:
    """
    Get current rock-paper-scissors game status.

    Retrieves real-time game state including current scores, round number,
    game mode, and session statistics.

    Returns:
        JSONResponse: Current game status and statistics.

    Example:
        >>> response = await api_rps_game_status()
        >>> response.json()
        {
            'status': 'active',
            'current_round': 2,
            'scores': {'player': 1, 'computer': 1},
            'target_score': 3
        }
    """
    return JSONResponse(rps_game_service.get_game_status())


@app.post("/api/rps/submit")
async def api_submit_rps_gesture(file: UploadFile = File(...)) -> JSONResponse:
    """
    Submit player's hand gesture image for recognition.

    Accepts an uploaded image of the player's hand gesture (rock/paper/scissors),
    uses MediaPipe model to detect and recognize the gesture with high accuracy.

    Args:
        file (UploadFile): The uploaded image file containing hand gesture.

    Returns:
        JSONResponse: Recognition result with gesture type and confidence.

    Raises:
        HTTPException: If file is invalid, game not active, or recognition fails.

    Example:
        >>> response = await api_submit_rps_gesture(image_file)
        >>> response.json()
        {
            'status': 'success',
            'gesture': 'rock',
            'confidence': 0.98
        }
    """
    # é©—è­‰æª”æ¡ˆ
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›æª”æ¡ˆ")

    # é©—è­‰æª”æ¡ˆæ ¼å¼
    file_ext = os.path.splitext(file.filename.lower())[1]
    image_exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}

    if file_ext not in image_exts:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„åœ–ç‰‡æ ¼å¼: {file_ext}ï¼Œè«‹ä½¿ç”¨ JPG, PNG, BMP, WEBP"
        )

    # è®€å–æª”æ¡ˆ
    file_content = await file.read()
    if len(file_content) > MAX_UPLOAD_SIZE_BYTES:
        limit_mb = MAX_UPLOAD_SIZE_BYTES // (1024 * 1024)
        raise HTTPException(status_code=413, detail=f"æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± {limit_mb}MB")

    # å»ºç«‹è‡¨æ™‚æª”æ¡ˆ
    suffix = file_ext or ".jpg"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(file_content)
        temp_path = tmp.name

    try:
        # æäº¤æ‰‹å‹¢çµ¦éŠæˆ²æœå‹™
        result = rps_game_service.submit_player_gesture(temp_path)

        if result.get("status") == "error":
            raise HTTPException(status_code=400, detail=result.get("message"))

        return JSONResponse(result)

    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if os.path.exists(temp_path):
            os.unlink(temp_path)


# =============================================================================
# æ‰‹å‹¢è­˜åˆ¥ API è·¯ç”±
# =============================================================================

@app.post("/api/gesture/start")
async def api_start_gesture_detection(duration: Optional[int] = Form(None)) -> JSONResponse:
    """
    Start hand gesture detection process.

    Initiates real-time hand gesture recognition using computer vision.
    Detects various hand gestures and signs for interactive applications.

    Args:
        duration (Optional[int]): Detection duration in seconds. If None, runs continuously.

    Returns:
        JSONResponse: Status response with success/error information.

    Raises:
        HTTPException: If gesture detection service fails to start.

    Example:
        >>> response = await api_start_gesture_detection(60)
        >>> response.json()
        {'status': 'success', 'message': 'Gesture detection started'}
    """
    result = hand_gesture_service.start_gesture_detection(duration)
    if result.get("status") == "error":
        raise HTTPException(
            status_code=400, detail=result.get("message", "å•Ÿå‹•æ‰‹å‹¢æª¢æ¸¬å¤±æ•—"))
    return JSONResponse(result)


@app.post("/api/gesture/stop")
async def api_stop_gesture_detection() -> JSONResponse:
    """
    Stop hand gesture detection process.

    Terminates the currently running gesture detection process and releases
    associated resources.

    Returns:
        JSONResponse: Status response indicating successful stop or error details.

    Example:
        >>> response = await api_stop_gesture_detection()
        >>> response.json()
        {'status': 'success', 'message': 'Gesture detection stopped'}
    """
    result = hand_gesture_service.stop_gesture_detection()
    return JSONResponse(result)


@app.get("/api/gesture/status")
async def api_gesture_status() -> JSONResponse:
    """
    Get current hand gesture detection status.

    Retrieves the current state of the gesture detection service, including
    whether it's running, detection statistics, and current gesture information.

    Returns:
        JSONResponse: Current status information of gesture detection service.

    Example:
        >>> response = await api_gesture_status()
        >>> response.json()
        {'status': 'running', 'uptime': 30, 'current_gesture': 'thumbs_up'}
    """
    return JSONResponse(hand_gesture_service.get_detection_status())


@app.get("/api/gesture/current")
async def api_current_gesture() -> JSONResponse:
    """
    Get current detected hand gesture.

    Retrieves the most recently detected hand gesture from the camera feed.
    Useful for real-time gesture-based interactions.

    Returns:
        JSONResponse: Current gesture information with confidence score.

    Example:
        >>> response = await api_current_gesture()
        >>> response.json()
        {'gesture': 'peace_sign', 'confidence': 0.92, 'timestamp': 1640995200}
    """
    return JSONResponse(hand_gesture_service.get_current_gesture())


# =============================================================================
# AI ç•«å¸ƒ API è·¯ç”±
# =============================================================================

@app.post("/api/drawing/start")
async def api_start_drawing_session(
    mode: str = Form("index_finger"),
    color: str = Form("black"),
    auto_recognize: bool = Form(True)
) -> JSONResponse:
    """
    Start AI drawing recognition session.

    Initiates an interactive drawing session where users can draw on a virtual
    canvas using hand gestures. The system can automatically recognize drawings
    or allow manual recognition triggers.

    Args:
        mode (str): Drawing input mode - "index_finger" or "full_hand".
        color (str): Drawing color - "black", "red", "blue", etc.
        auto_recognize (bool): Whether to automatically recognize drawings.

    Returns:
        JSONResponse: Session initialization status with session details.

    Raises:
        HTTPException: If drawing service fails to start or invalid parameters.

    Example:
        >>> response = await api_start_drawing_session("index_finger", "blue", True)
        >>> response.json()
        {'status': 'success', 'session_id': 'draw123', 'canvas_size': [800, 600]}
    """
    result = drawing_service.start_drawing_session(mode, color, auto_recognize)
    if result.get("status") == "error":
        raise HTTPException(
            status_code=400, detail=result.get("message", "å•Ÿå‹•ç¹ªç•«æœƒè©±å¤±æ•—"))
    return JSONResponse(result)


@app.post("/api/drawing/stop")
async def api_stop_drawing_session() -> JSONResponse:
    """
    Stop AI drawing recognition session.

    Terminates the current drawing session and saves any final drawings.
    Cleans up canvas resources and gesture tracking.

    Returns:
        JSONResponse: Session termination status with final results.

    Example:
        >>> response = await api_stop_drawing_session()
        >>> response.json()
        {'status': 'success', 'drawings_saved': 3, 'session_duration': 120}
    """
    result = drawing_service.stop_drawing_session()
    return JSONResponse(result)


@app.get("/api/drawing/status")
async def api_drawing_status() -> JSONResponse:
    """
    Get current AI drawing session status.

    Retrieves real-time drawing session information including current canvas
    state, drawing statistics, and recognition results.

    Returns:
        JSONResponse: Current drawing session status and statistics.

    Example:
        >>> response = await api_drawing_status()
        >>> response.json()
        {
            'status': 'active',
            'current_stroke_count': 45,
            'recognized_shapes': ['circle', 'square'],
            'canvas_size': [800, 600]
        }
    """
    return JSONResponse(drawing_service.get_drawing_status())


@app.post("/api/drawing/recognize")
async def api_recognize_drawing() -> JSONResponse:
    """
    Manually trigger AI recognition of current drawing.

    Forces immediate analysis of the current canvas content using AI vision
    models to identify shapes, objects, or patterns in the drawing.

    Returns:
        JSONResponse: Recognition results with identified elements and confidence.

    Example:
        >>> response = await api_recognize_drawing()
        >>> response.json()
        {
            'status': 'success',
            'recognized': 'house',
            'confidence': 0.87,
            'bounding_box': [100, 150, 300, 250]
        }
    """
    result = drawing_service.recognize_current_drawing()
    return JSONResponse(result)


@app.post("/api/drawing/clear")
async def api_clear_canvas() -> JSONResponse:
    """
    Clear the drawing canvas.

    Resets the virtual canvas to a blank state, removing all drawn content
    while preserving session settings and recognition history.

    Returns:
        JSONResponse: Canvas clear operation status.

    Example:
        >>> response = await api_clear_canvas()
        >>> response.json()
        {'status': 'success', 'message': 'Canvas cleared', 'previous_drawings': 2}
    """
    result = drawing_service.clear_canvas()
    return JSONResponse(result)





@app.websocket("/ws/rps")
async def websocket_rps(websocket: WebSocket) -> None:
    """
    WebSocket endpoint for real-time RPS game updates.

    Establishes a persistent WebSocket connection to stream rock-paper-scissors
    game results in real-time. Clients receive live updates as the game progresses.

    Args:
        websocket (WebSocket): The WebSocket connection instance.

    Note:
        Connection automatically handles cleanup on client disconnect.
        Only RPS-related messages are forwarded to this endpoint.
    """
    await websocket.accept()
    queue = await status_broadcaster.register()
    try:
        while True:
            message = await queue.get()
            if message.get("channel") == "rps_game":
                await websocket.send_json(message)
    except WebSocketDisconnect:
        pass
    finally:
        await status_broadcaster.unregister(queue)


@app.websocket("/ws/rps/stream")
async def websocket_rps_stream(websocket: WebSocket) -> None:
    """
    WebSocket å³æ™‚æ‰‹å‹¢è¾¨è­˜ç«¯é»ï¼ˆç”¨æ–¼ RPS éŠæˆ²ï¼‰

    æ¥æ”¶å®¢æˆ¶ç«¯ç™¼é€çš„å½±åƒå¹€ï¼Œä½¿ç”¨ MediaPipe é€²è¡Œå³æ™‚æ‰‹å‹¢è¾¨è­˜ï¼Œ
    ä¸¦å°‡è¾¨è­˜çµæœå³æ™‚è¿”å›çµ¦å®¢æˆ¶ç«¯ã€‚

    æ”¯æŒçš„è¨Šæ¯æ ¼å¼:
    - å®¢æˆ¶ç«¯ç™¼é€: {"type": "frame", "image": "base64_data", "timestamp": 123.45}
    - æœå‹™å™¨è¿”å›: {"type": "result", "gesture": "rock", "confidence": 0.96, ...}

    Args:
        websocket (WebSocket): WebSocketé€£æ¥å¯¦ä¾‹
    """
    await websocket.accept()
    logger.info("âœ… RPS ä¸²æµé€£æ¥å·²å»ºç«‹")

    try:
        while True:
            # æ¥æ”¶å®¢æˆ¶ç«¯æ¶ˆæ¯
            data = await websocket.receive_json()

            # è™•ç†å¿ƒè·³è¨Šæ¯
            if data.get("type") == "ping":
                await websocket.send_text("pong")
                continue

            if data.get("type") != "frame":
                message_type = data.get("type", "æœªå®šç¾©")
                await websocket.send_json({
                    "type": "error",
                    "message": f"ä¸æ”¯æŒçš„æ¶ˆæ¯é¡å‹: {message_type}"
                })
                continue

            # è§£æ base64 å½±åƒæ•¸æ“š
            image_data = data.get("image", "")
            timestamp = data.get("timestamp", 0)

            try:
                # è™•ç† base64 å½±åƒæ•¸æ“š
                if image_data.startswith("data:image/"):
                    # ç§»é™¤ data URL å‰ç¶´
                    image_data = image_data.split(",")[1]

                # è§£ç¢¼ base64
                image_bytes = base64.b64decode(image_data)

                # è½‰æ›ç‚º numpy arrayï¼ˆç›´æ¥åœ¨è¨˜æ†¶é«”ä¸­è™•ç†ï¼Œä¸éœ€è¦è‡¨æ™‚æª”æ¡ˆï¼‰
                import cv2
                nparr = np.frombuffer(image_bytes, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                if img is None:
                    await websocket.send_json({
                        "type": "error",
                        "message": "ç„¡æ³•è§£ç¢¼åœ–ç‰‡"
                    })
                    continue

                # ä½¿ç”¨ MediaPipe è¾¨è­˜æ‰‹å‹¢
                gesture, confidence = rps_game_service.detector.detect(img)

                # æº–å‚™å›æ‡‰
                result = {
                    "type": "result",
                    "gesture": gesture.value,
                    "confidence": float(confidence),
                    "timestamp": timestamp,
                    "is_valid": gesture.value != "unknown"
                }

                # ç™¼é€è¾¨è­˜çµæœ
                await websocket.send_json(result)

            except Exception as e:
                logger.exception("å½±åƒè¾¨è­˜éŒ¯èª¤: %s", e)
                await websocket.send_json({
                    "type": "error",
                    "message": f"å½±åƒè¾¨è­˜éŒ¯èª¤: {str(e)}"
                })

    except WebSocketDisconnect:
        logger.info("âŒ RPS ä¸²æµé€£æ¥å·²æ–·é–‹")
    except Exception as e:
        logger.exception("WebSocket éŒ¯èª¤: %s", e)
    finally:
        logger.info("ğŸ”Œ RPS ä¸²æµé€£æ¥é—œé–‰")


@app.websocket("/ws/gesture")
async def websocket_gesture(websocket: WebSocket) -> None:
    """
    WebSocket endpoint for real-time hand gesture updates.

    Establishes a persistent WebSocket connection to stream hand gesture
    detection results in real-time. Clients receive live updates as gestures
    are detected from camera feeds.

    Args:
        websocket (WebSocket): The WebSocket connection instance.

    Note:
        Connection automatically handles cleanup on client disconnect.
        Only gesture-related messages are forwarded to this endpoint.
    """
    await websocket.accept()
    queue = await status_broadcaster.register()
    try:
        while True:
            message = await queue.get()
            if message.get("channel") == "gesture":
                await websocket.send_json(message)
    except WebSocketDisconnect:
        pass
    finally:
        await status_broadcaster.unregister(queue)


@app.websocket("/ws/drawing")
async def websocket_drawing(websocket: WebSocket) -> None:
    """
    WebSocket endpoint for real-time drawing and AI recognition updates.

    Establishes a persistent WebSocket connection to stream drawing session
    and AI recognition results in real-time. Clients receive live updates as
    drawings are created and recognized.

    Args:
        websocket (WebSocket): The WebSocket connection instance.

    Note:
        Connection automatically handles cleanup on client disconnect.
        Only drawing-related messages are forwarded to this endpoint.
    """
    await websocket.accept()
    queue = await status_broadcaster.register()
    try:
        while True:
            message = await queue.get()
            if message.get("channel") == "drawing":
                await websocket.send_json(message)
    except WebSocketDisconnect:
        pass
    finally:
        await status_broadcaster.unregister(queue)

@app.websocket("/ws/drawing/gesture")
async def websocket_drawing_gesture(websocket: WebSocket) -> None:
    """
    WebSocket endpoint for real-time gesture-based drawing.

    Handles interactive gesture drawing where users can draw on a virtual canvas
    using hand gestures detected from camera frames. Processes camera frames in
    real-time to detect finger positions and translate them into drawing actions.

    Supported message types:
    - Client â†’ Server:
        - {"type": "open", "client_id": "unique_id"} - Open WebSocket connection
        - {"type": "start_gesture_drawing", "mode": "gesture_control", "color": "blue", "canvas_size": [720, 1280]}
        - {"type": "camera_frame", "image": "base64_data", "timestamp": 123.45}
        - {"type": "stop_drawing"} - Stop drawing session
        - {"type": "close"} - Close WebSocket connection

    - Server â†’ Client:
        - {"type": "opened", "session_id": "ws_gesture_12345", "status": "ready"}
        - {"type": "connection_confirmed", "client_id": "unique_id", "status": "active"}
        - {"type": "drawing_started", "session_id": "gesture_12345", "canvas_size": [720, 1280]}
        - {"type": "gesture_status", "current_gesture": "drawing", "fingers_up": [false, true, false, false, false]}
        - {"type": "canvas_update", "canvas_base64": "data:image/png;base64,...", "stroke_count": 15}
        - {"type": "recognition_result", "recognized_shape": "circle", "confidence": 0.87}
        - {"type": "drawing_stopped", "session_id": "gesture_12345", "final_recognition": {...}}
        - {"type": "closed", "reason": "client_request"}
        - {"type": "error", "message": "MediaPipe initialization failed"}

    Args:
        websocket (WebSocket): The WebSocket connection instance for gesture drawing.

    Note:
        This endpoint processes camera frames and performs gesture recognition
        for interactive drawing. Requires MediaPipe to be properly initialized.
    """
    await websocket.accept()

    # WebSocket session state
    ws_session_id = f"ws_gesture_{int(asyncio.get_event_loop().time() * 1000)}"
    gesture_session_active = False
    session_id = None
    drawing_mode = "gesture_control"
    client_id = None

    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "opened",
            "session_id": ws_session_id,
            "status": "ready",
            "message": "WebSocket connection established for gesture drawing"
        })

        while True:
            # Receive client message
            data = await websocket.receive_json()
            message_type = data.get("type", "")

            if message_type == "open":
                # Handle explicit WebSocket open request
                client_id = data.get("client_id", f"client_{int(asyncio.get_event_loop().time() * 1000)}")
                await websocket.send_json({
                    "type": "connection_confirmed",
                    "session_id": ws_session_id,
                    "client_id": client_id,
                    "status": "active"
                })

            elif message_type == "start_gesture_drawing":
                # Start gesture drawing session
                mode = data.get("mode", "gesture_control")
                color = data.get("color", "black")
                canvas_size = data.get("canvas_size", [640, 480])

                # If there's already an active session for this WebSocket, stop it first
                if gesture_session_active:
                    drawing_service.stop_drawing_session()
                    gesture_session_active = False

                # Start drawing session (WebSocket mode - no camera needed)
                result = drawing_service.start_drawing_session(
                    mode=mode,
                    color=color,
                    auto_recognize=True,
                    websocket_mode=True
                )

                if result.get("status") == "error":
                    # If session already exists, try to stop it and restart
                    if "å·²åœ¨é€²è¡Œä¸­" in result.get("message", ""):
                        drawing_service.stop_drawing_session()
                        # Try again after stopping
                        result = drawing_service.start_drawing_session(
                            mode=mode,
                            color=color,
                            auto_recognize=True,
                            websocket_mode=True
                        )

                if result.get("status") == "error":
                    await websocket.send_json({
                        "type": "error",
                        "message": result.get("message", "Failed to start gesture drawing"),
                        "timestamp": data.get("timestamp", 0)
                    })
                else:
                    gesture_session_active = True
                    drawing_mode = mode
                    session_id = f"gesture_{int(data.get('timestamp', 0) * 1000)}"

                    await websocket.send_json({
                        "type": "drawing_started",
                        "session_id": session_id,
                        "canvas_size": canvas_size,
                        "timestamp": data.get("timestamp", 0)
                    })

            elif message_type == "camera_frame" and gesture_session_active:
                # Process camera frame for gesture drawing
                image_data = data.get("image", "")
                timestamp = data.get("timestamp", 0)

                try:
                    # Decode base64 image
                    if image_data.startswith("data:image/"):
                        image_data = image_data.split(",")[1]

                    image_bytes = base64.b64decode(image_data)

                    # Process frame through drawing service
                    result = drawing_service.process_frame_for_gesture_drawing(
                        frame_data=image_bytes,
                        mode=drawing_mode
                    )

                    # Send the processing result back to client
                    await websocket.send_json(result)

                except Exception as e:
                    await websocket.send_json({
                        "type": "error",
                        "message": f"Frame processing error: {str(e)}",
                        "timestamp": timestamp
                    })

            elif message_type == "change_color" and gesture_session_active:
                # Handle color change during drawing
                new_color = data.get("color", "black")
                timestamp = data.get("timestamp", 0)

                try:
                    # Validate color
                    valid_colors = ["black", "red", "green", "blue", "yellow", "purple", "cyan", "white"]
                    if new_color not in valid_colors:
                        await websocket.send_json({
                            "type": "error",
                            "message": f"ç„¡æ•ˆçš„é¡è‰²: {new_color}ï¼Œæ”¯æ´çš„é¡è‰²: {', '.join(valid_colors)}",
                            "timestamp": timestamp
                        })
                    else:
                        # Change drawing color
                        drawing_service.change_drawing_color(new_color)
                        await websocket.send_json({
                            "type": "color_changed",
                            "color": new_color,
                            "message": f"ç¹ªç•«é¡è‰²å·²æ›´æ”¹ç‚º {new_color}",
                            "timestamp": timestamp
                        })

                except Exception as e:
                    await websocket.send_json({
                        "type": "error",
                        "message": f"é¡è‰²è®Šæ›´éŒ¯èª¤: {str(e)}",
                        "timestamp": timestamp
                    })

            elif message_type == "stop_drawing":
                # Stop gesture drawing session
                if gesture_session_active:
                    result = drawing_service.stop_drawing_session()
                    gesture_session_active = False

                    await websocket.send_json({
                        "type": "drawing_stopped",
                        "session_id": session_id,
                        "final_recognition": result.get("final_recognition", {}),
                        "timestamp": data.get("timestamp", 0)
                    })
                else:
                    await websocket.send_json({
                        "type": "error",
                        "message": "No active gesture drawing session",
                        "timestamp": data.get("timestamp", 0)
                    })

            elif message_type == "close":
                # Handle explicit WebSocket close request
                if gesture_session_active:
                    drawing_service.stop_drawing_session()
                    gesture_session_active = False

                await websocket.send_json({
                    "type": "closed",
                    "session_id": ws_session_id,
                    "reason": "client_request",
                    "timestamp": data.get("timestamp", 0)
                })
                break  # Exit the message loop to close the connection

            elif message_type == "ping":
                # Handle heartbeat ping - respond with pong
                await websocket.send_json({
                    "type": "pong",
                    "timestamp": data.get("timestamp", 0)
                })

            elif message_type == "pong":
                # Handle heartbeat pong - acknowledge silently
                pass

            else:
                # Unknown message type
                await websocket.send_json({
                    "type": "error",
                    "message": f"Unsupported message type: {message_type}",
                    "timestamp": data.get("timestamp", 0)
                })

    except WebSocketDisconnect:
        # Cleanup on disconnect
        if gesture_session_active:
            drawing_service.stop_drawing_session()
    except Exception as e:
        try:
            await websocket.send_json({
                "type": "error",
                "message": f"WebSocket error: {str(e)}"
            })
        except:
            pass


@app.websocket("/ws/action")
async def websocket_action(websocket: WebSocket) -> None:
    """
    WebSocket endpoint for real-time action detection updates.

    Establishes a persistent WebSocket connection to stream action detection
    results in real-time. Clients receive live updates as human actions are
    detected from camera feeds.

    Args:
        websocket (WebSocket): The WebSocket connection instance.

    Note:
        Connection automatically handles cleanup on client disconnect.
        Only action-related messages are forwarded to this endpoint.
    """
    await websocket.accept()
    queue = await status_broadcaster.register()
    try:
        while True:
            message = await queue.get()
            if message.get("channel") == "action":
                await websocket.send_json(message)
    except WebSocketDisconnect:
        pass
    finally:
        await status_broadcaster.unregister(queue)


@app.websocket("/ws/emotion")
async def websocket_emotion(websocket: WebSocket) -> None:
    """
    WebSocket å³æ™‚æƒ…ç·’åˆ†æç«¯é»

    æ¥æ”¶å®¢æˆ¶ç«¯ç™¼é€çš„å½±åƒå¹€ï¼Œä½¿ç”¨DeepFaceé€²è¡Œå³æ™‚æƒ…ç·’åˆ†æï¼Œ
    ä¸¦å°‡åˆ†æçµæœå³æ™‚è¿”å›çµ¦å®¢æˆ¶ç«¯ã€‚

    æ”¯æŒçš„è¨Šæ¯æ ¼å¼:
    - å®¢æˆ¶ç«¯ç™¼é€: {"type": "frame", "image": "base64_data", "timestamp": 123.45}
    - æœå‹™å™¨è¿”å›: {"type": "result", "emotion_zh": "é–‹å¿ƒ", "confidence": 0.96, ...}

    Args:
        websocket (WebSocket): WebSocketé€£æ¥å¯¦ä¾‹

    Note:
        WebSocket æœ¬èº«å°±æ˜¯ä¸²æµå”è­°ï¼Œä¸éœ€è¦é¡å¤–çš„ /stream å¾Œç¶´
    """
    await websocket.accept()

    try:
        while True:
            # æ¥æ”¶å®¢æˆ¶ç«¯æ¶ˆæ¯
            data = await websocket.receive_json()

            # è™•ç†å¿ƒè·³è¨Šæ¯
            if data.get("type") == "ping":
                await websocket.send_text("pong")
                continue

            if data.get("type") != "frame":
                message_type = data.get("type", "æœªå®šç¾©")
                await websocket.send_json({
                    "type": "error",
                    "message": f"ä¸æ”¯æŒçš„æ¶ˆæ¯é¡å‹: {message_type}",
                    "received_data": str(data)[:200]  # åªé¡¯ç¤ºå‰200å­—ç¬¦ä»¥é¿å…éé•·
                })
                continue

            # è§£æbase64å½±åƒæ•¸æ“š
            image_data = data.get("image", "")
            timestamp = data.get("timestamp", 0)

            try:
                # è™•ç†base64å½±åƒæ•¸æ“š
                if image_data.startswith("data:image/"):
                    # ç§»é™¤data URLå‰ç¶´
                    image_data = image_data.split(",")[1]

                # è§£ç¢¼base64
                image_bytes = base64.b64decode(image_data)

                # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_file:
                    tmp_file.write(image_bytes)
                    temp_path = tmp_file.name

                try:
                    # ä½¿ç”¨DeepFaceåˆ†ææƒ…ç·’
                    result = emotion_service.analyze_image_deepface(temp_path)

                    # æ·»åŠ æ™‚é–“æˆ³å’Œé¡å‹
                    result.update({
                        "type": "result",
                        "timestamp": timestamp,
                        "frame_time": timestamp
                    })

                    # ç™¼é€åˆ†æçµæœ
                    await websocket.send_json(result)

                finally:
                    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                    if os.path.exists(temp_path):
                        os.unlink(temp_path)

            except Exception as e:
                await websocket.send_json({
                    "type": "error",
                    "message": f"å½±åƒåˆ†æéŒ¯èª¤: {str(e)}",
                    "timestamp": timestamp
                })

    except WebSocketDisconnect:
        pass
    except Exception as e:
        try:
            await websocket.send_json({
                "type": "error",
                "message": f"WebSocketéŒ¯èª¤: {str(e)}"
            })
        except:
            pass
